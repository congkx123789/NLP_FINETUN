# âœ… CÃ i Ä‘áº·t hoÃ n táº¥t!

## MÃ´i trÆ°á»ng Ä‘Ã£ Ä‘Æ°á»£c thiáº¿t láº­p thÃ nh cÃ´ng

- **Conda Environment**: `unsloth_env` (Python 3.10)
- **GPU**: NVIDIA GeForce RTX 5060 Ti âœ…
- **CUDA Version**: 12.8 âœ…
- **PyTorch Version**: 2.9.1+cu128 âœ…
- **Unsloth**: ÄÃ£ cÃ i Ä‘áº·t vÃ  sáºµn sÃ ng âœ…

## CÃ¡ch sá»­ dá»¥ng

### 1. KÃ­ch hoáº¡t mÃ´i trÆ°á»ng má»—i láº§n má»Ÿ Terminal:

```bash
conda activate unsloth_env
```

Hoáº·c náº¿u conda chÆ°a Ä‘Æ°á»£c khá»Ÿi Ä‘á»™ng tá»± Ä‘á»™ng:

```bash
source ~/miniconda3/etc/profile.d/conda.sh
conda activate unsloth_env
```

### 2. Cháº¡y training script:

```bash
cd /home/alida/Documents/Cursor/NLP_fine_tun
python scripts/train_unsloth.py --direction en-vi
```

Hoáº·c cho chiá»u Viá»‡t-Anh:

```bash
python scripts/train_unsloth.py --direction vi-en
```

## LÆ°u Ã½ vá» Flash Attention 2

Flash Attention 2 chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t vÃ¬ cáº§n CUDA toolkit. Tuy nhiÃªn, **Unsloth Ä‘Ã£ cÃ³ tá»‘i Æ°u hÃ³a riÃªng** vÃ  sáº½ hoáº¡t Ä‘á»™ng tá»‘t mÃ  khÃ´ng cáº§n Flash Attention 2.

Náº¿u báº¡n muá»‘n cÃ i Flash Attention 2 sau (Ä‘á»ƒ tÄƒng tá»‘c thÃªm), báº¡n cáº§n:
1. CÃ i Ä‘áº·t CUDA toolkit tá»« NVIDIA
2. Thiáº¿t láº­p biáº¿n mÃ´i trÆ°á»ng `CUDA_HOME`
3. Cháº¡y: `pip install flash-attn --no-build-isolation`

## Kiá»ƒm tra cÃ i Ä‘áº·t

Äá»ƒ kiá»ƒm tra láº¡i má»i thá»© Ä‘Ã£ sáºµn sÃ ng:

```bash
conda activate unsloth_env
python -c "import torch; print(f'GPU: {torch.cuda.get_device_name(0)}'); print(f'CUDA: {torch.version.cuda}'); import unsloth; print('Unsloth OK!')"
```

## CÃ¡c thÆ° viá»‡n Ä‘Ã£ cÃ i Ä‘áº·t

- âœ… Unsloth (vá»›i tá»‘i Æ°u hÃ³a 2-3x tá»‘c Ä‘á»™)
- âœ… PyTorch 2.9.1 vá»›i CUDA 12.8
- âœ… Transformers, Datasets, PEFT, TRL
- âœ… BitsAndBytes (cho quantization)
- âœ… XFormers
- âœ… Táº¥t cáº£ cÃ¡c thÆ° viá»‡n há»— trá»£ cáº§n thiáº¿t

---

**Báº¡n Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ báº¯t Ä‘áº§u training! ğŸš€**




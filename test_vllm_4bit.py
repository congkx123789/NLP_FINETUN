#!/usr/bin/env python3
"""
Test vLLM á»Ÿ cháº¿ Ä‘á»™ 4-bit (bitsandbytes) vá»›i Qwen2.5-7B-Medical.

Cháº¡y:
    cd /home/alida/Documents/Cursor/NLP_fine_tun
    python test_vllm_4bit.py
"""

from vllm import LLM, SamplingParams
import time

# ÄÆ°á»ng dáº«n model Full-Bin cá»§a báº¡n
MODEL_PATH = "/home/alida/Documents/Cursor/NLP_fine_tun/final_models/Qwen2.5-7B-Medical-Full-Bin"


def main():
    print("â³ Äang khá»Ÿi Ä‘á»™ng vLLM cháº¿ Ä‘á»™ 4-bit (bitsandbytes)...")

    llm = LLM(
        model=MODEL_PATH,
        # NÃ©n 4-bit báº±ng bitsandbytes
        quantization="bitsandbytes",
        load_format="bitsandbytes",
        dtype="float16",             # TÃ­nh toÃ¡n 16-bit
        gpu_memory_utilization=0.9,  # Táº­n dá»¥ng 90% VRAM
        max_model_len=2048,
        tensor_parallel_size=1,
        trust_remote_code=True,
    )

    prompts = [
        "Translate to Vietnamese: The patient has a severe headache.",
        "Translate to English: Bá»‡nh nhÃ¢n bá»‹ Ä‘au bá»¥ng dá»¯ dá»™i.",
    ]

    sampling_params = SamplingParams(temperature=0.3, max_tokens=128)

    print("ðŸš€ Báº®T Äáº¦U CHáº Y...")
    start = time.time()
    outputs = llm.generate(prompts, sampling_params)
    end = time.time()

    print(f"âœ… Xong! Tá»•ng thá»i gian: {end - start:.2f}s")
    print("âš¡ vLLM 4-bit thÆ°á»ng sáº½ nhanh hÆ¡n Unsloth thuáº§n tÃºy trÃªn batch lá»›n.")
    for o in outputs:
        print(f">> {o.outputs[0].text.strip()}")


if __name__ == "__main__":
    main()



#!/usr/bin/env python3
"""
Test nhanh model Ä‘Ã£ merge báº±ng vLLM (khuyÃªn dÃ¹ng Ä‘á»ƒ Ä‘áº¡t tá»‘c Ä‘á»™ cao).

HÆ°á»›ng dáº«n:
    pip install vllm
    python scripts/test_fast_vllm.py
"""

import time
from vllm import LLM, SamplingParams

# ÄÆ°á»ng dáº«n tuyá»‡t Ä‘á»‘i Ä‘áº¿n model Ä‘Ã£ merge
MODEL_PATH = "/home/alida/Documents/Cursor/NLP_fine_tun/final_models/Qwen2.5-7B-Medical-Full-Bin"


def main():
    print(f"â³ Äang load model vÃ o vLLM tá»«: {MODEL_PATH}...")

    # Load model (vLLM tá»± quáº£n lÃ½ VRAM ráº¥t hiá»‡u quáº£)
    llm = LLM(
        model=MODEL_PATH,
        dtype="float16",            # Cháº¡y 16-bit cho nháº¹
        gpu_memory_utilization=0.85,  # DÃ¹ng 85% VRAM, chá»«a Ã­t cho mÃ n hÃ¬nh
        trust_remote_code=True,
    )

    # Cáº¥u hÃ¬nh sinh chá»¯
    sampling_params = SamplingParams(temperature=0.3, max_tokens=200)

    # Danh sÃ¡ch cÃ¢u test (cÃ³ thá»ƒ nhÃ©t nhiá»u cÃ¢u tÃ¹y Ã½)
    prompts = [
        "Translate to Vietnamese: The patient presents with severe abdominal pain.",
        "Translate to Vietnamese: Dosage: Take 500mg twice daily after meals.",
        "Translate to Vietnamese: MRI scan reveals a mass in the left lung.",
        "Translate to English: Bá»‡nh nhÃ¢n cÃ³ tiá»n sá»­ dá»‹ á»©ng vá»›i cÃ¡c loáº¡i háº£i sáº£n.",
        "Translate to English: Chá»‰ Ä‘á»‹nh: Pháº«u thuáº­t ná»™i soi cáº¯t ruá»™t thá»«a.",
    ]

    print("\n" + "=" * 50)
    print("ğŸš€ Báº®T Äáº¦U CHáº Y...")
    start_time = time.time()

    # Cháº¡y inference (batch)
    outputs = llm.generate(prompts, sampling_params)

    end_time = time.time()
    print(f"âœ… ÄÃ£ xong! Tá»•ng thá»i gian: {end_time - start_time:.2f} giÃ¢y")
    print("=" * 50 + "\n")

    # In káº¿t quáº£
    for output in outputs:
        print(f"ğŸ”¹ Input: {output.prompt}")
        print(f"ğŸ”¸ Output: {output.outputs[0].text.strip()}")
        print("-" * 30)


if __name__ == "__main__":
    main()



#!/usr/bin/env python3
"""
Ch·∫°y th·ª≠ model ƒë√£ merge 16-bit:
    final_models/Qwen2.5-7B-Medical-Full-Bin

D√πng tr·ª±c ti·∫øp Transformers (kh√¥ng qua Unsloth), v·ªõi format prompt:

    Translate the following English text to Vietnamese:
    Input: ...
    Output:

ho·∫∑c

    Translate the following Vietnamese text to English:
    Input: ...
    Output:
"""

import argparse

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer


def build_prompt(direction: str, text: str) -> str:
    if direction == "en-vi":
        instr = "Translate the following English text to Vietnamese:"
    else:
        instr = "Translate the following Vietnamese text to English:"
    return f"{instr}\nInput: {text}\nOutput:"


def main():
    parser = argparse.ArgumentParser(
        description="Ch·∫°y th·ª≠ Qwen2.5-7B-Medical-Full-Bin ƒë·ªÉ d·ªãch en-vi / vi-en."
    )
    parser.add_argument(
        "--model-dir",
        type=str,
        default="final_models/Qwen2.5-7B-Medical-Full-Bin",
        help="ƒê∆∞·ªùng d·∫´n t·ªõi model ƒë√£ merge 16-bit.",
    )
    parser.add_argument(
        "--direction",
        type=str,
        default="en-vi",
        choices=["en-vi", "vi-en"],
        help="Chi·ªÅu d·ªãch.",
    )
    parser.add_argument(
        "--text",
        type=str,
        required=True,
        help="C√¢u/ƒëo·∫°n vƒÉn ngu·ªìn c·∫ßn d·ªãch.",
    )
    parser.add_argument(
        "--max-new-tokens",
        type=int,
        default=128,
        help="S·ªë token t·ªëi ƒëa model sinh ra.",
    )

    args = parser.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"üì• Loading full model from: {args.model_dir} (device={device})")

    tokenizer = AutoTokenizer.from_pretrained(args.model_dir)
    model = AutoModelForCausalLM.from_pretrained(
        args.model_dir,
        torch_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16,
        device_map="auto" if device == "cuda" else None,
    )
    model.eval()

    prompt = build_prompt(args.direction, args.text)
    print("\n====== PROMPT G·ª¨I V√ÄO MODEL ======")
    print(prompt)
    print("==================================\n")

    inputs = tokenizer(
        [prompt],
        return_tensors="pt",
        padding=False,
        truncation=True,
        max_length=2048,
    ).to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=args.max_new_tokens,
            do_sample=False,
            use_cache=True,
            repetition_penalty=1.1,
            no_repeat_ngram_size=2,
            eos_token_id=tokenizer.eos_token_id,
        )

    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]

    # C·∫Øt ph·∫ßn sau "Output:"
    marker = "Output:"
    idx = decoded.rfind(marker)
    if idx != -1:
        translation = decoded[idx + len(marker) :].strip()
    else:
        translation = decoded.strip()

    print("====== B·∫¢N D·ªäCH ======")
    print(translation)
    print("======================")


if __name__ == "__main__":
    main()



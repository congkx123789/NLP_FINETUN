#!/usr/bin/env python3
"""
Script training vá»›i Unsloth - Tá»‘i Æ°u cho RTX 5060 Ti 16GB GDDR7
"""

import argparse
import json
from pathlib import Path
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments, TrainerCallback
from datasets import load_dataset
import torch

def load_json_dataset(file_path):
    """Load dataset tá»« JSON file"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # Convert sang format cho Unsloth
    texts = []
    for item in data:
        instruction = item.get('instruction', '')
        input_text = item.get('input', '')
        output_text = item.get('output', '')
        
        # Format: instruction + input + output
        text = f"{instruction}\nInput: {input_text}\nOutput: {output_text}"
        texts.append({"text": text})
    
    return texts

class SimpleLogCallback(TrainerCallback):
    """In ra loss & learning rate theo thá»i gian thá»±c (má»—i logging_steps)."""

    def on_log(self, args, state, control, logs=None, **kwargs):
        if not logs:
            return
        step = state.global_step
        epoch = logs.get("epoch", state.epoch)
        loss = logs.get("loss", logs.get("train_loss", None))
        lr = logs.get("learning_rate", None)
        if loss is not None or lr is not None:
            msg = f"ğŸ“‰ Step {step}"
            if epoch is not None:
                msg += f" | epoch={epoch:.2f}"
            if loss is not None:
                msg += f" | loss={loss:.4f}"
            if lr is not None:
                msg += f" | lr={lr:.6f}"
            print(msg, flush=True)


def main():
    parser = argparse.ArgumentParser(description="Train vá»›i Unsloth - Tá»‘i Æ°u tá»‘c Ä‘á»™")
    parser.add_argument("--train-file", type=str, default=None, help="File train JSON")
    parser.add_argument("--val-file", type=str, default=None, help="File validation JSON")
    parser.add_argument("--output-dir", type=str, default=None, help="Output directory")
    # direction:
    #   - en-vi : chá»‰ train chiá»u Anh -> Viá»‡t
    #   - vi-en : chá»‰ train chiá»u Viá»‡t -> Anh
    #   - mixed: train cáº£ 2 chiá»u trong 1 model (dá»¯ liá»‡u Ä‘Ã£ mix sáºµn, dÃ¹ng instruction lÃ m "cÃ´ng táº¯c")
    parser.add_argument(
        "--direction",
        type=str,
        default="en-vi",
        choices=["en-vi", "vi-en", "mixed"],
        help="HÆ°á»›ng dá»‹ch. 'mixed' dÃ¹ng file JSON Ä‘Ã£ gá»™p cáº£ en-vi & vi-en.",
    )
    parser.add_argument("--max-seq-length", type=int, default=2048, help="Max sequence length")
    parser.add_argument("--batch-size", type=int, default=4, help="Batch size per device")
    parser.add_argument("--gradient-accumulation-steps", type=int, default=4, help="Gradient accumulation steps")
    parser.add_argument("--lora-rank", type=int, default=8, help="LoRA rank")
    parser.add_argument("--epochs", type=float, default=1.0, help="Number of epochs (náº¿u khÃ´ng dÃ¹ng --max-steps)")
    parser.add_argument("--max-steps", type=int, default=None, help="Train theo sá»‘ bÆ°á»›c (Æ°u tiÃªn hÆ¡n epochs náº¿u Ä‘Æ°á»£c set)")
    parser.add_argument("--use-torch-compile", action="store_true", help="Enable torch.compile (requires no 4-bit quantization)")
    parser.add_argument("--quantization", type=str, default="4bit", choices=["4bit", "8bit", "none"], help="Quantization type")
    
    args = parser.parse_args()
    
    # Tá»± Ä‘á»™ng tÃ¬m file náº¿u khÃ´ng Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh
    base_dir = Path(__file__).parent.parent
    if args.train_file is None:
        if args.direction == "en-vi":
            args.train_file = str(base_dir / "data/vlsp_medical_en_vi_train.json")
            args.val_file = args.val_file or str(base_dir / "data/vlsp_medical_en_vi_val.json")
            args.output_dir = args.output_dir or str(base_dir / "saves/qwen2_5-7b/unsloth/en_vi")
        elif args.direction == "vi-en":
            args.train_file = str(base_dir / "data/vlsp_medical_vi_en_train.json")
            args.val_file = args.val_file or str(base_dir / "data/vlsp_medical_vi_en_val.json")
            args.output_dir = args.output_dir or str(base_dir / "saves/qwen2_5-7b/unsloth/vi_en")
        else:  # mixed
            # YÃŠU Cáº¦U: Ä‘Ã£ táº¡o sáºµn cÃ¡c file:
            #   data/vlsp_medical_mixed_train.json
            #   data/vlsp_medical_mixed_val.json
            args.train_file = str(base_dir / "data/vlsp_medical_mixed_train.json")
            args.val_file = args.val_file or str(base_dir / "data/vlsp_medical_mixed_val.json")
            args.output_dir = args.output_dir or str(base_dir / "saves/qwen2_5-7b/unsloth/mixed")
    
    # Táº¡o output directory náº¿u chÆ°a cÃ³
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    
    print("ğŸš€ Khá»Ÿi Ä‘á»™ng training vá»›i Unsloth (Tá»‘i Æ°u RTX 5060 Ti)")
    print("=" * 60)
    print(f"ğŸ“‚ Train file: {args.train_file}")
    print(f"ğŸ“‚ Val file: {args.val_file}")
    print(f"ğŸ“‚ Output dir: {args.output_dir}")
    print(f"ğŸ“‚ Direction: {args.direction}")
    print("")
    
    # 1. Load Model vá»›i quantization tÃ¹y chá»n vÃ  Flash Attention 2
    print("ğŸ“¥ Äang load model vá»›i Flash Attention 2...")
    print("   âš¡ Flash Attention 2 tá»± Ä‘á»™ng báº­t náº¿u GPU há»— trá»£ (RTX 5060 Ti cÃ³ há»— trá»£)")
    
    # Xá»­ lÃ½ quantization dá»±a trÃªn yÃªu cáº§u torch.compile
    # LÆ°u Ã½: torch.compile() KHÃ”NG tÆ°Æ¡ng thÃ­ch vá»›i Báº¤T Ká»² quantization nÃ o khi dÃ¹ng PEFT
    if args.use_torch_compile:
        print("   âš ï¸  torch.compile Ä‘Æ°á»£c báº­t - Bá» QUA quantization (khÃ´ng tÆ°Æ¡ng thÃ­ch vá»›i PEFT)")
        print("   â„¹ï¸  Sáº½ dÃ¹ng bf16 + gradient checkpointing Ä‘á»ƒ tiáº¿t kiá»‡m VRAM")
        load_in_4bit = False
        load_in_8bit = False
    else:
        load_in_4bit = (args.quantization == "4bit")
        load_in_8bit = (args.quantization == "8bit")
    
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="/home/alida/Documents/Cursor/NLP_fine_tun/models/Qwen2.5-7B",
        max_seq_length=args.max_seq_length,
        dtype=None,
        load_in_4bit=load_in_4bit,
        load_in_8bit=load_in_8bit,
        # Flash Attention 2 tá»± Ä‘á»™ng báº­t trong Unsloth náº¿u GPU há»— trá»£
    )
    
    # 2. Config LoRA vá»›i Unsloth optimizations
    print("âš™ï¸  Äang cáº¥u hÃ¬nh LoRA vá»›i Unsloth optimizations...")
    model = FastLanguageModel.get_peft_model(
        model,
        r=args.lora_rank,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_alpha=args.lora_rank * 2,
        lora_dropout=0,
        bias="none",
        use_gradient_checkpointing="unsloth",  # Unsloth's optimized gradient checkpointing
        random_state=3407,
    )
    
    # 3. Torch Compile Ä‘á»ƒ tá»‘i Æ°u cho RTX 5060 Ti (Blackwell architecture)
    if args.use_torch_compile:
        try:
            print("âš¡ Äang báº­t torch.compile Ä‘á»ƒ tá»‘i Æ°u cho RTX 5060 Ti (Blackwell)...")
            print("   âš ï¸  LÆ°u Ã½: torch.compile KHÃ”NG tÆ°Æ¡ng thÃ­ch vá»›i quantization khi dÃ¹ng PEFT")
            print("   âœ… ÄÃ£ bá» quantization - model sáº½ dÃ¹ng bf16 + gradient checkpointing")
            print("   ğŸ’¡ Náº¿u thiáº¿u VRAM, cÃ³ thá»ƒ cáº§n giáº£m batch-size")
            model = torch.compile(model, mode="reduce-overhead", fullgraph=True)
            print("âœ… torch.compile Ä‘Ã£ Ä‘Æ°á»£c báº­t! Tá»‘c Ä‘á»™ sáº½ tÄƒng thÃªm 10-20%")
        except Exception as e:
            print(f"âš ï¸  torch.compile khÃ´ng kháº£ dá»¥ng: {e}")
            print("   Training váº«n sáº½ cháº¡y bÃ¬nh thÆ°á»ng vá»›i Unsloth (Ä‘Ã£ nhanh 2-3x)")
    else:
        print("â„¹ï¸  torch.compile chÆ°a Ä‘Æ°á»£c báº­t")
        print("   Äá»ƒ báº­t, thÃªm flag --use-torch-compile (sáº½ bá» quantization)")
        print("   Unsloth Ä‘Ã£ tá»‘i Æ°u sáºµn, tá»‘c Ä‘á»™ váº«n ráº¥t nhanh")
    
    
    # 4. Load dataset
    print("ğŸ“Š Äang load dataset...")
    # Load JSON files directly
    train_dataset = load_dataset("json", data_files=args.train_file, split="train")
    
    # Convert to format cho Unsloth
    def format_text(example):
        instruction = example.get('instruction', '')
        input_text = example.get('input', '')
        output_text = example.get('output', '')
        text = f"{instruction}\nInput: {input_text}\nOutput: {output_text}"
        return {"text": text}
    
    train_dataset = train_dataset.map(format_text, remove_columns=train_dataset.column_names)
    
    eval_dataset = None
    if args.val_file:
        eval_dataset = load_dataset("json", data_files=args.val_file, split="train")
        eval_dataset = eval_dataset.map(format_text, remove_columns=eval_dataset.column_names)
    
    # 5. Training Arguments tá»‘i Æ°u cho RTX 5060 Ti + dá»¯ liá»‡u lá»›n
    # Æ¯u tiÃªn train theo steps náº¿u --max-steps Ä‘Æ°á»£c set, ngÆ°á»£c láº¡i dÃ¹ng epochs nhÆ° bÃ¬nh thÆ°á»ng.
    training_args_kwargs = dict(
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        warmup_steps=100,
        learning_rate=2e-4,
        fp16=False,
        bf16=True,  # Báº¯t buá»™c cho RTX 5060 Ti
        logging_steps=10,
        optim="adamw_8bit",  # Tiáº¿t kiá»‡m VRAM
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir=args.output_dir,
        dataloader_num_workers=4,  # Tá»‘i Æ°u CPU, trÃ¡nh ngháº½n I/O
        save_strategy="steps",
        save_steps=500,
        eval_strategy="steps" if eval_dataset else "no",
        eval_steps=2000 if eval_dataset else None,
    )
    if args.max_steps is not None:
        training_args_kwargs["max_steps"] = args.max_steps
    else:
        training_args_kwargs["num_train_epochs"] = args.epochs

    training_args = TrainingArguments(**training_args_kwargs)
    
    # 6. Trainer vá»›i packing (TÄƒng tá»‘c 2-3x)
    print("ğŸ¯ Báº¯t Ä‘áº§u training vá»›i packing=True (TÄƒng tá»‘c 2-3x)...")
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        dataset_text_field="text",
        max_seq_length=args.max_seq_length,
        dataset_num_proc=2,
        packing=True,  # âš¡ QUAN TRá»ŒNG: Gá»™p dá»¯ liá»‡u Ä‘á»ƒ train siÃªu nhanh
        args=training_args,
        callbacks=[SimpleLogCallback()],
    )
    
    # 6. Train
    train_output = trainer.train()
    
    # 7. Thá»‘ng kÃª thá»i gian train (dá»… Ä‘á»c)
    metrics = getattr(train_output, "metrics", None) or {}
    # Fallback: láº¥y tá»« trainer.state náº¿u cáº§n
    if not metrics and trainer.state.log_history:
        for log in reversed(trainer.state.log_history):
            if "train_runtime" in log:
                metrics = log
                break
    train_runtime = float(metrics.get("train_runtime", 0.0))
    train_epochs = float(metrics.get("epoch", args.epochs))
    train_loss = float(metrics.get("train_loss", -1.0))
    total_seconds = int(train_runtime)
    hours = total_seconds // 3600
    minutes = (total_seconds % 3600) // 60
    seconds = total_seconds % 60
    print(f"â±ï¸  Thá»i gian train: {train_runtime:.2f} giÃ¢y (~{hours}h {minutes}m {seconds}s) | epochs={train_epochs} | train_loss={train_loss:.4f}")
    
    # 8. Save
    print("ğŸ’¾ Äang lÆ°u model...")
    model.save_pretrained(args.output_dir)
    tokenizer.save_pretrained(args.output_dir)
    
    print(f"âœ… HoÃ n thÃ nh! Model Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {args.output_dir}")

if __name__ == "__main__":
    main()


#!/usr/bin/env python3
"""
Script benchmark tá»‘c Ä‘á»™ Unsloth trÃªn RTX 5060 Ti vá»›i:
- 4-bit quantization
- FastLanguageModel.for_inference
- Batch size = 8

Cháº¡y:
    cd /home/alida/Documents/Cursor/NLP_fine_tun
    python scripts/max_speed_unsloth.py
"""

from unsloth import FastLanguageModel
import torch
import time

# --- Cáº¤U HÃŒNH ---
MODEL_PATH = "/home/alida/Documents/Cursor/NLP_fine_tun/final_models/Qwen2.5-7B-Medical-Full-Bin"


def main():
    print(f"â³ Äang load model vá»›i Flash Attention 2 tá»«: {MODEL_PATH}...")

    # 1. Load Model (Báº¯t buá»™c 4-bit Ä‘á»ƒ nháº¹ vÃ  nhanh trÃªn card 16GB)
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=MODEL_PATH,
        max_seq_length=2048,
        dtype=None,
        load_in_4bit=True,
    )

    # 2. KÃCH HOáº T TÄ‚NG Tá»C (QUAN TRá»ŒNG NHáº¤T)
    FastLanguageModel.for_inference(model)

    # 3. Dá»¯ liá»‡u test (Giáº£ láº­p 16 cÃ¢u há»i y táº¿)
    prompts = [
        "Translate to Vietnamese: The patient presents with severe abdominal pain.",
        "Translate to Vietnamese: Dosage: Take 500mg twice daily after meals.",
        "Translate to Vietnamese: MRI scan reveals a mass in the left lung.",
        "Translate to English: Bá»‡nh nhÃ¢n cÃ³ tiá»n sá»­ dá»‹ á»©ng vá»›i cÃ¡c loáº¡i háº£i sáº£n.",
        "Translate to English: Chá»‰ Ä‘á»‹nh: Pháº«u thuáº­t ná»™i soi cáº¯t ruá»™t thá»«a.",
        "Translate to Vietnamese: Acute kidney failure is a rapid loss of kidney function.",
        "Translate to Vietnamese: The doctor prescribed antibiotics for the infection.",
        "Translate to English: Bá»‡nh nhÃ¢n bá»‹ gÃ£y xÆ°Æ¡ng Ä‘Ã¹i pháº£i do tai náº¡n giao thÃ´ng.",
    ] * 2  # NhÃ¢n Ä‘Ã´i lÃªn thÃ nh 16 cÃ¢u Ä‘á»ƒ test kháº£ nÄƒng chá»‹u táº£i

    print(f"\nðŸš€ Äang xá»­ lÃ½ {len(prompts)} cÃ¢u vá»›i batch_size = 8...")
    start_time = time.time()

    # 4. CHáº Y BATCH (Thay vÃ¬ vÃ²ng láº·p for tá»«ng cÃ¢u)
    batch_size = 8
    results = []

    # ThÃªm padding side left cho decoder (Báº¯t buá»™c khi batching)
    tokenizer.padding_side = "left"

    device = model.device

    for i in range(0, len(prompts), batch_size):
        batch = prompts[i : i + batch_size]

        # Tokenize 1 cá»¥c
        inputs = tokenizer(batch, return_tensors="pt", padding=True)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=128,
                use_cache=True,  # Báº¯t buá»™c True Ä‘á»ƒ nhanh
                pad_token_id=tokenizer.eos_token_id,
            )

        # Decode káº¿t quáº£
        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        results.extend(decoded)

    end_time = time.time()
    total_time = end_time - start_time

    print("=" * 50)
    print(f"âš¡ Tá»‘c Ä‘á»™ xá»­ lÃ½: {len(prompts) / total_time:.2f} cÃ¢u/giÃ¢y")
    print(f"â±ï¸ Tá»•ng thá»i gian: {total_time:.2f} giÃ¢y")
    print("=" * 50)

    # In thá»­ vÃ i káº¿t quáº£
    for res in results[:2]:
        print(f">> {res}")


if __name__ == "__main__":
    main()





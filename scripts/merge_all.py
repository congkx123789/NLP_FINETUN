#!/usr/bin/env python3
"""
Merge LoRA adapter Ä‘Ã£ train vÃ o base model Qwen2.5-7B Ä‘á»ƒ táº¡o:
1) Báº£n 16-bit merged (cho Python/vLLM/HuggingFace)
2) Báº£n GGUF (cho Ollama/LM Studio)
"""

from unsloth import FastLanguageModel
import torch  # noqa: F401 - giá»¯ láº¡i náº¿u sau nÃ y cáº§n
import os

# --- Cáº¤U HÃŒNH ---

# ÄÆ°á»ng dáº«n Ä‘áº¿n báº£n vÃ¡ báº¡n vá»«a train (Folder chá»©a adapter_model.safetensors)
adapter_path = "saves/qwen2_5-7b/unsloth/mixed_maxsteps10000_fa2"

# TÃªn folder Ä‘áº§u ra
output_16bit = "final_models/Qwen2.5-7B-Medical-16bit"  # Folder chá»©a model 16bit
output_gguf = "final_models/Qwen2.5-7B-Medical-GGUF"    # Folder chá»©a file GGUF


def main():
    print(f"â³ Äang load model tá»«: {adapter_path}...")

    # 1. Load Model & Tokenizer (4-bit Ä‘á»ƒ tiáº¿t kiá»‡m VRAM khi xá»­ lÃ½)
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=adapter_path,
        max_seq_length=2048,
        dtype=None,
        load_in_4bit=True,
    )

    # Táº¡o folder cha náº¿u chÆ°a cÃ³
    os.makedirs("final_models", exist_ok=True)

    # ---------------------------------------------------------
    # BÆ¯á»šC 1: XUáº¤T Báº¢N 16-BIT (Merged Float16)
    # ---------------------------------------------------------
    print("\n" + "=" * 50)
    print("ğŸ’¾ BÆ¯á»šC 1: Äang hÃ n báº£n vÃ¡ ra Ä‘á»‹nh dáº¡ng 16-bit (safetensors)...")
    print(f"   LÆ°u táº¡i: {output_16bit}")
    print("=" * 50)

    model.save_pretrained_merged(
        output_16bit,
        tokenizer,
        save_method="merged_16bit",  # HÃ n cháº¿t vÃ o model gá»‘c
    )

    print("âœ… ÄÃ£ xong báº£n 16-bit!")

    # ---------------------------------------------------------
    # BÆ¯á»šC 2: XUáº¤T Báº¢N GGUF (Cho Ollama/LM Studio)
    # ---------------------------------------------------------
    print("\n" + "=" * 50)
    print("ğŸ’¾ BÆ¯á»šC 2: Äang chuyá»ƒn Ä‘á»•i sang GGUF (Quantization q4_k_m)...")
    print("   QuÃ¡ trÃ¬nh nÃ y sáº½ tá»‘n nhiá»u RAM vÃ  CPU, hÃ£y kiÃªn nháº«n...")
    print(f"   LÆ°u táº¡i: {output_gguf}")
    print("=" * 50)

    model.save_pretrained_gguf(
        output_gguf,
        tokenizer,
        quantization_method="q4_k_m",  # Chuáº©n cÃ¢n báº±ng nháº¥t hiá»‡n nay
    )

    print("\nğŸ‰ HOÃ€N Táº¤T TOÃ€N Bá»˜ QUÃ TRÃŒNH!")
    print(f"1. Báº£n 16-bit: náº±m trong '{output_16bit}'")
    print(f"2. Báº£n GGUF : náº±m trong '{output_gguf}'")


if __name__ == "__main__":
    main()



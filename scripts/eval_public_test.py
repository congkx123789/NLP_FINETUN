#!/usr/bin/env python3
"""
ÄÃ¡nh giÃ¡ / cháº¡y inference model Unsloth (Qwen2.5-7B) trÃªn file public_test.json.
- DÃ¹ng FastLanguageModel (Unsloth) -> Flash Attention 2 tá»± báº­t náº¿u GPU há»— trá»£.
- Input:  data/public_test.json  (id, direction, source, target)
- Output: results/public_test_predictions.tsv
"""

import argparse
import json
import time
from pathlib import Path
from typing import List, Dict

import torch
from unsloth import FastLanguageModel


def build_prompt(item: Dict) -> str:
    """
    Táº¡o prompt ÄÃšNG Vá»šI FORMAT ÄÃƒ TRAIN:
        instruction
        Input: ...
        Output:
    (y há»‡t nhÆ° trong train_unsloth.py)
    """
    direction = item.get("direction")
    source = item.get("source", "")

    if direction == "en-vi":
        instruction = "Translate the following English text to Vietnamese:"
    else:
        instruction = "Translate the following Vietnamese text to English:"

    # LÃºc train: "Output: {output_text}" -> lÃºc test chá»‰ Ä‘á»ƒ "Output:" cho model tá»± Ä‘iá»n
    prompt = f"{instruction}\nInput: {source}\nOutput:"
    return prompt


def extract_answer(full_text: str) -> str:
    """
    TÃ¡ch pháº§n dá»‹ch sau 'Output:' khá»i toÃ n bá»™ chuá»—i generate.
    """
    marker = "Output:"
    idx = full_text.rfind(marker)
    if idx == -1:
        return full_text.strip()
    return full_text[idx + len(marker) :].strip()


def run_inference(
    model_dir: str,
    test_file: str,
    output_tsv: str,
    batch_size: int = 8,
    max_new_tokens: int = 256,
    resume: bool = False,
) -> None:
    device = "cuda" if torch.cuda.is_available() else "cpu"
    start_time = time.time()
    print(f"ğŸ“¥ Äang load model tá»«: {model_dir} (device={device})")

    # DÃ¹ng Unsloth model (4bit) Ä‘á»ƒ giá»¯ Flash Attention 2 + tá»‘i Æ°u VRAM
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_dir,
        max_seq_length=2048,
        dtype=None,
        load_in_4bit=True,
    )

    model.eval()

    # Load test json
    test_path = Path(test_file)
    with test_path.open("r", encoding="utf-8") as f:
        data = json.load(f)
    print(f"ğŸ“Š Sá»‘ cÃ¢u test: {len(data)}")

    # Chuáº©n bá»‹ output
    out_path = Path(output_tsv)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    print(f"ğŸ’¾ Sáº½ ghi káº¿t quáº£ vÃ o: {out_path}")

    # LÆ°u thÃªm káº¿t quáº£ dáº¡ng JSON cho dá»… Ä‘á»c / phÃ¢n tÃ­ch
    json_results: List[Dict] = []
    json_path = out_path.with_suffix(".json")

    # Xá»­ lÃ½ logic resume: náº¿u Ä‘Ã£ cÃ³ TSV/JSON thÃ¬ tiáº¿p tá»¥c tá»« dÃ²ng káº¿ tiáº¿p
    start_index = 0
    tsv_mode = "w"
    write_header = True

    if resume and out_path.exists():
        lines: List[str]
        with out_path.open("r", encoding="utf-8") as f_prev:
            lines = f_prev.readlines()
        if len(lines) > 1:
            # ÄÃ£ cÃ³ sáºµn prediction trÆ°á»›c Ä‘Ã³
            start_index = len(lines) - 1  # trá»« header
            tsv_mode = "a"
            write_header = False
            print(f"ğŸ” Resume tá»« máº«u thá»© {start_index} (dá»±a trÃªn TSV hiá»‡n cÃ³).")
        else:
            start_index = 0
            tsv_mode = "w"
            write_header = True

        # Náº¿u Ä‘Ã£ cÃ³ file JSON cÅ© thÃ¬ load vÃ o Ä‘á»ƒ ná»‘i thÃªm
        if json_path.exists():
            import json as _json

            with json_path.open("r", encoding="utf-8") as jf_prev:
                try:
                    json_results = _json.load(jf_prev)
                except Exception:
                    json_results = []
    else:
        start_index = 0
        tsv_mode = "w"
        write_header = True

    with out_path.open(tsv_mode, encoding="utf-8") as fw:
        # Header
        if write_header:
            fw.write("id\tdirection\tsource\ttarget_ref\tprediction\n")

        # Batch inference
        total = len(data)
        if start_index >= total:
            print(f"âœ… Táº¥t cáº£ {total} máº«u Ä‘Ã£ cÃ³ prediction, khÃ´ng cáº§n resume.")
            return

        for start in range(start_index, total, batch_size):
            end = min(start + batch_size, total)
            batch_items: List[Dict] = data[start:end]

            prompts = [build_prompt(item) for item in batch_items]
            inputs = tokenizer(
                prompts,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=2048,
            ).to(device)

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=False,  # greedy cho MT á»•n Ä‘á»‹nh
                    use_cache=True,
                    # ğŸš« Chá»‘ng láº·p / thoÃ¡i hÃ³a vÄƒn báº£n
                    repetition_penalty=1.2,
                    no_repeat_ngram_size=2,
                    eos_token_id=tokenizer.eos_token_id,
                )

            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)

            for item, prompt, full_out in zip(batch_items, prompts, decoded):
                # Trong full_out cÃ³ cáº£ prompt + answer -> tÃ¡ch pháº§n answer
                # PhÃ²ng trÆ°á»ng há»£p tokenizer bá» bá»›t prompt, váº«n fallback báº±ng marker 'Output:'
                if full_out.startswith(prompt):
                    answer = full_out[len(prompt) :].strip()
                else:
                    answer = extract_answer(full_out)

                pred_clean = answer.replace(chr(9), " ")
                src_clean = item.get("source", "").replace(chr(9), " ")
                tgt_clean = item.get("target", "").replace(chr(9), " ")

                # Ghi TSV (phá»¥c vá»¥ ná»™p bÃ i / tÃ­nh BLEU)
                fw.write(
                    f"{item.get('id')}\t{item.get('direction')}\t"
                    f"{src_clean}\t"
                    f"{tgt_clean}\t"
                    f"{pred_clean}\n"
                )

                # Gom Ä‘á»ƒ xuáº¥t thÃªm file JSON dá»… Ä‘á»c
                json_results.append(
                    {
                        "id": item.get("id"),
                        "direction": item.get("direction"),
                        "source": src_clean,
                        "target_ref": tgt_clean,
                        "prediction": pred_clean,
                    }
                )

            print(f"âœ… ÄÃ£ xá»­ lÃ½ {end}/{total} cÃ¢u", flush=True)

    elapsed = time.time() - start_time
    mins = elapsed / 60.0
    print(f"ğŸ‰ HoÃ n thÃ nh! Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u táº¡i: {out_path}")
    print(f"â±ï¸ Tá»•ng thá»i gian inference: {elapsed:.1f} giÃ¢y (~{mins:.2f} phÃºt) cho {total} cÃ¢u.")

    # Ghi thÃªm file JSON Ä‘áº¹p cho báº¡n dá»… xem báº±ng editor / Jupyter
    import json as _json

    with json_path.open("w", encoding="utf-8") as jf:
        _json.dump(json_results, jf, ensure_ascii=False, indent=2)
    print(f"ğŸ“„ Äá»“ng thá»i Ä‘Ã£ lÆ°u báº£n JSON dá»… Ä‘á»c táº¡i: {json_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Cháº¡y inference model Unsloth (Qwen2.5-7B) trÃªn public_test.json"
    )
    parser.add_argument(
        "--model-dir",
        type=str,
        default="saves/qwen2_5-7b/unsloth/mixed_maxsteps10000_fa2",
        help="ThÆ° má»¥c model Unsloth Ä‘Ã£ fine-tune (adapter).",
    )
    parser.add_argument(
        "--test-file",
        type=str,
        default="data/public_test.json",
        help="File JSON test (format: id, direction, source, target).",
    )
    parser.add_argument(
        "--output-tsv",
        type=str,
        default="results/public_test_predictions.tsv",
        help="File TSV output chá»©a prediction.",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=8,
        help="Batch size khi inference.",
    )
    parser.add_argument(
        "--max-new-tokens",
        type=int,
        default=256,
        help="Sá»‘ token tá»‘i Ä‘a model sinh ra.",
    )
    parser.add_argument(
        "--resume",
        action="store_true",
        help="Tiáº¿p tá»¥c tá»« TSV/JSON hiá»‡n cÃ³, khÃ´ng cháº¡y láº¡i tá»« Ä‘áº§u.",
    )

    args = parser.parse_args()
    run_inference(
        model_dir=args.model_dir,
        test_file=args.test_file,
        output_tsv=args.output_tsv,
        batch_size=args.batch_size,
        max_new_tokens=args.max_new_tokens,
        resume=args.resume,
    )


if __name__ == "__main__":
    main()




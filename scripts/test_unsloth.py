#!/usr/bin/env python3
"""
Test Qwen2.5-7B-Medical (Ä‘Ã£ merge) vá»›i Unsloth trÃªn bá»™ public_test.*
vÃ  lÆ°u cÃ¢u tráº£ lá»i cá»§a model ra file JSON.

VÃ­ dá»¥ cháº¡y:
    # Dá»‹ch Anh -> Viá»‡t
    python scripts/test_unsloth.py --direction en-vi --max-samples 50

    # Dá»‹ch Viá»‡t -> Anh
    python scripts/test_unsloth.py --direction vi-en --max-samples 50
"""

import argparse
import json
from pathlib import Path

import torch
from unsloth import FastLanguageModel


def build_prompt(direction: str, src_sentence: str) -> str:
    """Táº¡o prompt giá»‘ng style training (instruction + Input + Output)."""
    if direction == "en-vi":
        instruction = "Dá»‹ch Ä‘oáº¡n vÄƒn sau tá»« tiáº¿ng Anh sang tiáº¿ng Viá»‡t trong lÄ©nh vá»±c y táº¿."
    else:
        instruction = "Dá»‹ch Ä‘oáº¡n vÄƒn sau tá»« tiáº¿ng Viá»‡t sang tiáº¿ng Anh trong lÄ©nh vá»±c y táº¿."

    return f"{instruction}\nInput: {src_sentence}\nOutput:"


def main():
    parser = argparse.ArgumentParser(description="Test Qwen2.5-7B-Medical vá»›i Unsloth vÃ  lÆ°u káº¿t quáº£ ra JSON.")
    parser.add_argument(
        "--direction",
        type=str,
        default="en-vi",
        choices=["en-vi", "vi-en"],
        help="HÆ°á»›ng dá»‹ch: en-vi hoáº·c vi-en.",
    )
    parser.add_argument(
        "--max-samples",
        type=int,
        default=None,
        help="Giá»›i háº¡n sá»‘ cÃ¢u test (máº·c Ä‘á»‹nh: dÃ¹ng toÃ n bá»™ file).",
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="TÃªn file JSON output (máº·c Ä‘á»‹nh: tá»± sinh theo direction).",
    )
    parser.add_argument(
        "--max-new-tokens",
        type=int,
        default=256,
        help="Sá»‘ token tá»‘i Ä‘a model sinh ra cho má»—i cÃ¢u.",
    )
    args = parser.parse_args()

    base_dir = Path(__file__).parent.parent

    # Chá»‰ Ä‘Æ°á»ng Ä‘áº¿n model Ä‘Ã£ merge
    model_dir = base_dir / "final_models/Qwen2.5-7B-Medical-Full-Bin"

    # File test nguá»“n/Ä‘Ã­ch
    test_dir = base_dir / "Test"
    if args.direction == "en-vi":
        src_path = test_dir / "public_test.en.txt"
        tgt_path = test_dir / "public_test.vi.txt"
        default_output = test_dir / "public_test.en-vi.pred.jsonl"
    else:
        src_path = test_dir / "public_test.vi.txt"
        tgt_path = test_dir / "public_test.en.txt"
        default_output = test_dir / "public_test.vi-en.pred.jsonl"

    output_path = Path(args.output) if args.output else default_output

    print("ğŸ“‚ Model dir :", model_dir)
    print("ğŸ“‚ Source    :", src_path)
    print("ğŸ“‚ Target    :", tgt_path)
    print("ğŸ“‚ Output    :", output_path)

    # Äá»c dá»¯ liá»‡u test
    with open(src_path, "r", encoding="utf-8") as f_src:
        src_lines = [line.strip() for line in f_src.readlines()]
    with open(tgt_path, "r", encoding="utf-8") as f_tgt:
        tgt_lines = [line.strip() for line in f_tgt.readlines()]

    assert len(src_lines) == len(tgt_lines), "Sá»‘ dÃ²ng source vÃ  target khÃ´ng khá»›p!"

    if args.max_samples is not None:
        src_lines = src_lines[: args.max_samples]
        tgt_lines = tgt_lines[: args.max_samples]

    print(f"ğŸ”¢ Sá»‘ cÃ¢u sáº½ test: {len(src_lines)}")

    # Load model vá»›i Unsloth (4bit Ä‘á»ƒ tiáº¿t kiá»‡m VRAM khi test)
    print("ğŸ§  Äang load model merged vá»›i Unsloth (4bit)...")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=str(model_dir),
        max_seq_length=2048,
        dtype=None,
        load_in_4bit=True,
    )

    FastLanguageModel.for_inference(model)  # Báº­t cháº¿ Ä‘á»™ inference tá»‘i Æ°u

    device = model.device

    # Cháº¡y dá»‹ch tá»«ng cÃ¢u vÃ  lÆ°u JSONL
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f_out:
        for idx, (src, tgt) in enumerate(zip(src_lines, tgt_lines), start=1):
            prompt = build_prompt(args.direction, src)

            inputs = tokenizer(
                [prompt],
                return_tensors="pt",
            )
            inputs = {k: v.to(device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=args.max_new_tokens,
                    do_sample=False,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id,
                )

            generated = outputs[0][inputs["input_ids"].shape[1] :]
            pred = tokenizer.decode(generated, skip_special_tokens=True).strip()

            record = {
                "id": idx,
                "direction": args.direction,
                "source": src,
                "target": tgt,
                "prediction": pred,
            }
            f_out.write(json.dumps(record, ensure_ascii=False) + "\n")

            if idx % 50 == 0 or idx == 1 or idx == len(src_lines):
                print(f"âœ… ÄÃ£ xá»­ lÃ½ {idx}/{len(src_lines)} cÃ¢u")

    print(f"ğŸ‰ HoÃ n táº¥t! Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u táº¡i: {output_path}")


if __name__ == "__main__":
    main()



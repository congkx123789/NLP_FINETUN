#!/usr/bin/env python3
"""
Ch·∫°y Qwen2.5-7B-Medical-Full-Bin v·ªõi Unsloth + FlashAttention 2 (FA2)
====================================================================

- Model g·ªëc (ƒë√£ merge 16-bit) n·∫±m t·∫°i:
    final_models/Qwen2.5-7B-Medical-Full-Bin

- Script n√†y:
    * D√πng Unsloth `FastLanguageModel.from_pretrained` ƒë·ªÉ b·∫≠t FA2 t·ª± ƒë·ªông.
    * T√πy ch·ªçn `load_in_4bit=True` ƒë·ªÉ gi·∫£m VRAM v√† tƒÉng t·ªëc inference.
    * D√πng ƒë√∫ng format prompt nh∆∞ khi train:
          {instruction}
          Input: {text}
          Output:
"""

import argparse

import torch
from unsloth import FastLanguageModel


def build_prompt(direction: str, text: str) -> str:
    """T·∫°o prompt ƒë√∫ng format ƒë√£ train."""
    if direction == "en-vi":
        instr = "Translate the following English text to Vietnamese:"
    else:
        instr = "Translate the following Vietnamese text to English:"
    return f"{instr}\nInput: {text}\nOutput:"


def main():
    parser = argparse.ArgumentParser(
        description="Ch·∫°y Qwen2.5-7B-Medical-Full-Bin v·ªõi Unsloth + FA2 ƒë·ªÉ d·ªãch en-vi / vi-en."
    )
    parser.add_argument(
        "--model-dir",
        type=str,
        default="final_models/Qwen2.5-7B-Medical-Full-Bin",
        help="ƒê∆∞·ªùng d·∫´n t·ªõi model ƒë√£ merge 16-bit.",
    )
    parser.add_argument(
        "--direction",
        type=str,
        default="en-vi",
        choices=["en-vi", "vi-en"],
        help="Chi·ªÅu d·ªãch.",
    )
    parser.add_argument(
        "--text",
        type=str,
        required=True,
        help="C√¢u/ƒëo·∫°n vƒÉn ngu·ªìn c·∫ßn d·ªãch.",
    )
    parser.add_argument(
        "--max-new-tokens",
        type=int,
        default=128,
        help="S·ªë token t·ªëi ƒëa model sinh ra.",
    )
    parser.add_argument(
        "--no-4bit",
        action="store_true",
        help="N·∫øu set, kh√¥ng d√πng 4-bit (load full 16-bit, t·ªën VRAM h∆°n, ch·∫≠m h∆°n).",
    )

    args = parser.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"üì• Loading full model (via Unsloth) from: {args.model_dir} (device={device})")

    load_in_4bit = not args.no_4bit

    # Unsloth s·∫Ω t·ª± b·∫≠t FlashAttention 2 n·∫øu GPU h·ªó tr·ª£ (RTX 5060 Ti c√≥ h·ªó tr·ª£)
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=args.model_dir,
        max_seq_length=2048,
        dtype=None,          # ƒë·ªÉ Unsloth t·ª± ch·ªçn bf16/fp16 ph√π h·ª£p
        load_in_4bit=load_in_4bit,
    )
    model.eval()

    prompt = build_prompt(args.direction, args.text)
    print("\n====== PROMPT G·ª¨I V√ÄO MODEL ======")
    print(prompt)
    print("==================================\n")

    inputs = tokenizer(
        [prompt],
        return_tensors="pt",
        padding=False,
        truncation=True,
        max_length=2048,
    ).to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=args.max_new_tokens,
            do_sample=False,
            use_cache=True,
            # Tham s·ªë ch·ªëng l·∫∑p / tho√°i h√≥a
            repetition_penalty=1.1,
            no_repeat_ngram_size=2,
            eos_token_id=tokenizer.eos_token_id,
        )

    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]

    # C·∫Øt ph·∫ßn sau "Output:"
    marker = "Output:"
    idx = decoded.rfind(marker)
    if idx != -1:
        translation = decoded[idx + len(marker) :].strip()
    else:
        translation = decoded.strip()

    print("====== B·∫¢N D·ªäCH ======")
    print(translation)
    print("======================")


if __name__ == "__main__":
    main()



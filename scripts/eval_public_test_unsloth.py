#!/usr/bin/env python3
"""
ÄÃ¡nh giÃ¡ model Qwen2.5-7B-Medical báº±ng Unsloth 4-bit trÃªn bá»™ public_test.json
(cáº£ hai chiá»u en-vi vÃ  vi-en), KHÃ”NG dÃ¹ng vLLM.

Nguá»“n:
    data/public_test.json  (Ä‘Ã£ chá»©a cáº£ en-vi & vi-en)

ÄÃ­ch:
    data/public_test.pred.jsonl

Má»—i dÃ²ng output:
    {
      "id": <int>,
      "direction": "en-vi" | "vi-en",
      "source": "...",
      "target": "...",        # tham chiáº¿u
      "prediction": "..."     # model dá»‹ch
    }

Cháº¡y:
    cd /home/alida/Documents/Cursor/NLP_fine_tun
    python scripts/eval_public_test_unsloth.py --max-samples 50
"""

import argparse
import json
from pathlib import Path

import torch
from unsloth import FastLanguageModel


def build_prompt(direction: str, source: str) -> str:
    """Táº¡o prompt giá»‘ng format training (instruction + Input/Output)."""
    if direction == "en-vi":
        instruction = "Dá»‹ch Ä‘oáº¡n vÄƒn sau tá»« tiáº¿ng Anh sang tiáº¿ng Viá»‡t trong lÄ©nh vá»±c y táº¿."
    else:
        instruction = "Dá»‹ch Ä‘oáº¡n vÄƒn sau tá»« tiáº¿ng Viá»‡t sang tiáº¿ng Anh trong lÄ©nh vá»±c y táº¿."

    return f"{instruction}\nInput: {source}\nOutput:"


def main():
    parser = argparse.ArgumentParser(description="Eval public_test.json báº±ng Unsloth 4-bit (khÃ´ng dÃ¹ng vLLM).")
    parser.add_argument(
        "--input",
        type=str,
        default="data/public_test.json",
        help="ÄÆ°á»ng dáº«n file JSON test (máº·c Ä‘á»‹nh: data/public_test.json).",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="data/public_test.pred.jsonl",
        help="ÄÆ°á»ng dáº«n file JSONL output (máº·c Ä‘á»‹nh: data/public_test.pred.jsonl).",
    )
    parser.add_argument(
        "--max-samples",
        type=int,
        default=None,
        help="Giá»›i háº¡n sá»‘ máº«u Ä‘á»ƒ test nhanh (máº·c Ä‘á»‹nh: dÃ¹ng toÃ n bá»™).",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=8,
        help="Batch size cho inference (máº·c Ä‘á»‹nh: 8).",
    )
    parser.add_argument(
        "--max-new-tokens",
        type=int,
        default=256,
        help="Sá»‘ token tá»‘i Ä‘a model sinh ra.",
    )
    args = parser.parse_args()

    base_dir = Path(__file__).parent.parent
    model_dir = base_dir / "final_models/Qwen2.5-7B-Medical-Full-Bin"

    input_path = base_dir / args.input
    output_path = base_dir / args.output

    print(f"ğŸ“‚ Model dir : {model_dir}")
    print(f"ğŸ“‚ Input     : {input_path}")
    print(f"ğŸ’¾ Output    : {output_path}")

    # Load dá»¯ liá»‡u test
    with open(input_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    if args.max_samples is not None:
        data = data[: args.max_samples]

    print(f"ğŸ”¢ Sá»‘ máº«u sáº½ test: {len(data)}")

    # Load model 4-bit vá»›i Unsloth
    print("ğŸ§  Äang load model merged vá»›i Unsloth (4-bit, khÃ´ng dÃ¹ng vLLM)...")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=str(model_dir),
        max_seq_length=2048,
        dtype=None,
        load_in_4bit=True,
    )

    FastLanguageModel.for_inference(model)
    device = model.device

    # Batching
    tokenizer.padding_side = "left"

    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f_out:
        batch_size = args.batch_size
        for i in range(0, len(data), batch_size):
            batch_samples = data[i : i + batch_size]

            prompts = [build_prompt(s["direction"], s["source"]) for s in batch_samples]

            inputs = tokenizer(
                prompts,
                return_tensors="pt",
                padding=True,
            )
            inputs = {k: v.to(device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=args.max_new_tokens,
                    use_cache=True,
                    pad_token_id=tokenizer.eos_token_id,
                )

            # Cáº¯t pháº§n má»›i sinh
            gen_seqs = outputs[:, inputs["input_ids"].shape[1] :]
            decoded = tokenizer.batch_decode(gen_seqs, skip_special_tokens=True)

            for sample, pred in zip(batch_samples, decoded):
                record = {
                    "id": sample.get("id"),
                    "direction": sample.get("direction"),
                    "source": sample.get("source"),
                    "target": sample.get("target"),
                    "prediction": pred.strip(),
                }
                f_out.write(json.dumps(record, ensure_ascii=False) + "\n")

            print(f"âœ… ÄÃ£ xá»­ lÃ½ {min(i + batch_size, len(data))}/{len(data)} máº«u", flush=True)

    print(f"ğŸ‰ HoÃ n táº¥t! Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u táº¡i: {output_path}")


if __name__ == "__main__":
    main()





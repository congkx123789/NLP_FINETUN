#!/usr/bin/env python3
"""
Export Qwen2.5-7B + LoRA (checkpoint-10000) thÃ nh 1 model Ä‘Ã£ hÃ n cháº¿t báº£n vÃ¡,
lÆ°u ra Ä‘Ä©a dáº¡ng .safetensors (hoáº·c .bin náº¿u cáº§n).

Cháº¡y:
    cd /home/alida/Documents/Cursor/NLP_fine_tun
    python scripts/export_to_bin.py
"""

from unsloth import FastLanguageModel

import torch  # noqa: F401 - giá»¯ láº¡i náº¿u sau nÃ y cáº§n
import os
from pathlib import Path


def main():
    # --- Cáº¤U HÃŒNH ---
    base_dir = Path(__file__).parent.parent

    # 1. ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c chá»©a báº£n vÃ¡ (nÆ¡i cÃ³ adapter_model.safetensors cá»§a checkpoint-10000)
    #    VÃ­ dá»¥: saves/qwen2_5-7b/unsloth/mixed_maxsteps10000_fa2/checkpoint-10000
    adapter_path = base_dir / "saves/qwen2_5-7b/unsloth/mixed_maxsteps10000_fa2/checkpoint-10000"

    # 2. ThÆ° má»¥c Ä‘áº§u ra (NÆ¡i sáº½ chá»©a model.safetensors hoáº·c pytorch_model.bin vÃ  vocab/tokenizer)
    output_dir = base_dir / "final_models/Qwen2.5-7B-Medical-Full-Bin"

    print(f"â³ Äang load Adapter tá»«: {adapter_path}...")

    # Load model gá»‘c + adapter
    # DÃ¹ng 4-bit Ä‘á»ƒ tiáº¿t kiá»‡m VRAM lÃºc merge
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=str(adapter_path),
        max_seq_length=2048,
        dtype=None,
        load_in_4bit=True,
    )

    print("\n" + "=" * 50)
    print("ğŸ’¾ ÄANG Há»¢P NHáº¤T (MERGE) VÃ€ LÆ¯U FILE...")
    print("   QuÃ¡ trÃ¬nh nÃ y sáº½ táº¡o ra file náº·ng khoáº£ng 15GB.")
    print("=" * 50)

    # Táº¡o thÆ° má»¥c Ä‘áº§u ra
    os.makedirs(output_dir, exist_ok=True)

    # --- QUAN TRá»ŒNG: Lá»†NH LÆ¯U RA Dáº NG BIN/SAFETENSORS ---
    # save_method="merged_16bit": Há»£p nháº¥t thÃ nh 1 khá»‘i float16
    model.save_pretrained_merged(
        str(output_dir),
        tokenizer,
        save_method="merged_16bit",
    )

    # LÆ°u Ã½:
    # - Máº·c Ä‘á»‹nh Unsloth sáº½ lÆ°u thÃ nh file Ä‘uÃ´i .safetensors (hiá»‡n Ä‘áº¡i hÆ¡n .bin)
    # - Äa sá»‘ cÃ¡c tool (vLLM, HuggingFace, Python) Ä‘á»u Ä‘á»c Ä‘Æ°á»£c .safetensors y há»‡t .bin
    #
    # Náº¿u Báº®T BUá»˜C pháº£i cáº§n file tÃªn lÃ  pytorch_model.bin, bá» comment block dÆ°á»›i:
    #
    # print("\nğŸ” Äang xuáº¥t thÃªm báº£n .bin (safe_serialization=False)...")
    # model.merge_and_unload()
    # model.save_pretrained(str(output_dir), safe_serialization=False)
    # tokenizer.save_pretrained(str(output_dir))

    print(f"\nâœ… XONG! HÃ£y kiá»ƒm tra thÆ° má»¥c: {output_dir}")
    print("Báº¡n sáº½ tháº¥y cÃ¡c file: config.json, model.safetensors (hoáº·c bin), vocab.json, tokenizer.json...")


if __name__ == "__main__":
    main()



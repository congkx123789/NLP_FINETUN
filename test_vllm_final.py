#!/usr/bin/env python3
"""
Test tá»‘c Ä‘á»™ vLLM vá»›i Qwen2.5-7B-Medical (full bin) trÃªn RTX 5060 Ti.

Cháº¡y:
    cd /home/alida/Documents/Cursor/NLP_fine_tun
    python test_vllm_final.py
"""

from vllm import LLM, SamplingParams
import time

# ÄÆ°á»ng dáº«n model Full-Bin cá»§a báº¡n
MODEL_PATH = "/home/alida/Documents/Cursor/NLP_fine_tun/final_models/Qwen2.5-7B-Medical-Full-Bin"


def main():
    print("â³ Äang khá»Ÿi Ä‘á»™ng vLLM...")

    # Cáº¥u hÃ¬nh tá»‘i Æ°u cho RTX 5060 Ti 16GB (nhÆ°ng váº«n cÃ³ nguy cÆ¡ OOM vá»›i fp16)
    llm = LLM(
        model=MODEL_PATH,
        dtype="float16",
        gpu_memory_utilization=0.9,  # DÃ¹ng 90% VRAM
        max_model_len=2048,          # Giá»›i háº¡n Ä‘á»™ dÃ i Ä‘á»ƒ tiáº¿t kiá»‡m nhá»›
        tensor_parallel_size=1,
        trust_remote_code=True,
    )

    prompts = [
        "Translate to Vietnamese: The patient has a severe headache.",
        "Translate to Vietnamese: Take 2 tablets twice daily.",
        "Translate to English: Bá»‡nh nhÃ¢n bá»‹ Ä‘au bá»¥ng dá»¯ dá»™i.",
    ]

    sampling_params = SamplingParams(temperature=0.3, max_tokens=128)

    print("ğŸš€ Báº®T Äáº¦U CHáº Y...")
    start = time.time()
    outputs = llm.generate(prompts, sampling_params)
    end = time.time()

    print(f"âœ… Xong! Tá»•ng thá»i gian: {end - start:.2f}s")
    for o in outputs:
        print(f"Input: {o.prompt}")
        print(f"Output: {o.outputs[0].text.strip()}")
        print("-" * 20)


if __name__ == "__main__":
    main()


